{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Module 5: Natural language processing**\n",
    "## DAT410\n",
    "\n",
    "### Group 29 \n",
    "### David Laessker, 980511-5012, laessker@chalmers.se\n",
    "\n",
    "### Oskar Palmgren, 010529-4714, oskarpal@chalmers.se\n",
    "\n",
    "\n",
    "\n",
    "We hereby declare that we have both actively participated in solving every exercise. All solutions are entirely our own work, without having taken part of other solutions.\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Reading and reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "In general, life would get easier if computers were able to take over some simple intellectual tasks that only humans are able to do, in order to automate things. In addition to translation, things like image and speech recognition has been resarched alot, and has a very large range of possibilities. The task of converting spoken language into text has seen approaches ranging from Hidden Markov models to deep neural networks, including CNNs and RNNs.\n",
    "\n",
    "### b)\n",
    "\n",
    "Systems that are rule-based explicitly use linguistic rules and dictionaries, while neural systems learn these linguistic patterns from large datasets. Both approaches aim to accurately translate languages by mapping structures and meanings, but through different means. The reason for the output of the two different models could be the same, but the reason why is competely different. But essentially they both reflect patterns of speech in a society, although one has learned them by observation, the other has had the rules/patterns defined for it.\n",
    "\n",
    "### c)\n",
    "\n",
    "In situations where there is not alot of data avalible the neural network or the statistical model may not perform very well. For example for the minority language Meänkieli that is spoken by around 70 000 people in northen parts of Sweden there is not alot of data for a statistical or neural model to train on, and it may be confused because the language is very close to Finnish and often used interchangably with Finnish and Swedish. Same goes for the language spoken in the Faroe Islands, Faroese. It is similar looking to icelandic and also spoken by around 70 000 people. In these case using a rule based model would most likely be the prefered method. Training on a statistical or neural model with small and often confusing data may lead to a broken och poorly working model. Whereas a rule based model will always obey the rules, even though it has a lack of versitiality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from html import unescape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "swe_eng_file_path = 'data/europarl-v7.sv-en.lc.sv'\n",
    "eng_swe_file_path = 'data\\europarl-v7.sv-en.lc.en'\n",
    "\n",
    "ger_eng_file_path = 'data\\europarl-v7.de-en.lc.de'\n",
    "eng_ger_file_path = 'data\\europarl-v7.de-en.lc.en'\n",
    "\n",
    "fre_eng_file_path = 'data\\europarl-v7.fr-en.lc.fr'\n",
    "eng_fre_file_path = 'data\\europarl-v7.fr-en.lc.en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a function that calculates the word frequency in a file. We go through the file line by line and update a word counter everytime a specific word appears. The result is a dictionary with all the words and their respective frequency.\n",
    "\n",
    "We noticed that punctuations (periods and commas) were in the top 10 in all languages. French also had HTML entity \"&apos\" (apostrohe), so we also added an argument to be able to calculate the word frequency without special symbols. However, this is only used in this task because these symbols may be important and handled differently in the translation model, for example punctuations for sentence boundary detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency(file, remove_symbols=False):\n",
    "    \n",
    "    word_counter = Counter()\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "    \n",
    "        for line in f:\n",
    "\n",
    "            if remove_symbols:\n",
    "                line = unescape(line)\n",
    "                line = re.sub(r'[^\\w\\s]', '', line)\n",
    "            \n",
    "            words = line.split()\n",
    "            word_counter.update(words)\n",
    "    \n",
    "    return word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 9648),\n",
       " ('att', 9181),\n",
       " (',', 8876),\n",
       " ('och', 7038),\n",
       " ('i', 5949),\n",
       " ('det', 5687),\n",
       " ('som', 5028),\n",
       " ('för', 4959),\n",
       " ('av', 4013),\n",
       " ('är', 3840)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swe_frequency = word_frequency(swe_eng_file_path)\n",
    "\n",
    "swe_top_10 = swe_frequency.most_common(10)\n",
    "\n",
    "swe_top_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 58790),\n",
       " (',', 42043),\n",
       " ('.', 29542),\n",
       " ('of', 28406),\n",
       " ('to', 26842),\n",
       " ('and', 21459),\n",
       " ('in', 18485),\n",
       " ('is', 13331),\n",
       " ('that', 13219),\n",
       " ('a', 13090)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_frequency1 = word_frequency(eng_swe_file_path)\n",
    "eng_frequency2 = word_frequency(eng_ger_file_path)\n",
    "eng_frequency3 = word_frequency(eng_fre_file_path)\n",
    "\n",
    "eng_total_frequency = eng_frequency1 + eng_frequency2 + eng_frequency3\n",
    "\n",
    "eng_top_10 = eng_total_frequency.most_common(10)\n",
    "\n",
    "eng_top_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 18549),\n",
       " ('die', 10521),\n",
       " ('.', 9733),\n",
       " ('der', 9374),\n",
       " ('und', 7028),\n",
       " ('in', 4175),\n",
       " ('zu', 3168),\n",
       " ('den', 2976),\n",
       " ('wir', 2863),\n",
       " ('daß', 2738)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ger_frequency = word_frequency(ger_eng_file_path)\n",
    "\n",
    "ger_top_10 = ger_frequency.most_common(10)\n",
    "\n",
    "ger_top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('&apos;', 16729),\n",
       " (',', 15402),\n",
       " ('de', 14520),\n",
       " ('la', 9746),\n",
       " ('.', 9734),\n",
       " ('et', 6619),\n",
       " ('l', 6536),\n",
       " ('le', 6174),\n",
       " ('les', 5585),\n",
       " ('à', 5500)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fre_frequency = word_frequency(fre_eng_file_path)\n",
    "\n",
    "fre_top_10 = fre_frequency.most_common(10)\n",
    "\n",
    "fre_top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 84870),\n",
       " ('the', 58812),\n",
       " ('.', 58657),\n",
       " ('of', 28414),\n",
       " ('to', 26843),\n",
       " ('in', 22808),\n",
       " ('and', 21463),\n",
       " ('&apos;', 20013),\n",
       " ('de', 17536),\n",
       " ('i', 14894)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eur_parl_frequency = swe_frequency + fre_frequency + ger_frequency + eng_total_frequency\n",
    "\n",
    "eur_parl_top_10 = eur_parl_frequency.most_common(10)\n",
    "\n",
    "eur_parl_top_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of speaker: 0.00193 %\n",
      "Probability of zebra: 0.0 %\n"
     ]
    }
   ],
   "source": [
    "words_amount = sum(eur_parl_frequency.values())\n",
    "\n",
    "speaker_probability = (eur_parl_frequency['speaker'] / words_amount ) * 100\n",
    "zebra_probability = (eur_parl_frequency['zebra'] / words_amount ) * 100\n",
    "\n",
    "# calculate probability for \"speaker\" and \"zebra\"\n",
    "print(f'Probability of speaker: {speaker_probability:.5f} %')\n",
    "print(f'Probability of zebra: {zebra_probability} %')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the most common words and probability without all the symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 58812),\n",
       " ('of', 28414),\n",
       " ('to', 26843),\n",
       " ('in', 22808),\n",
       " ('and', 21463),\n",
       " ('de', 17536),\n",
       " ('i', 14907),\n",
       " ('a', 14853),\n",
       " ('is', 13340),\n",
       " ('that', 13219)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Without symbols\n",
    "\n",
    "file_paths = ['data/europarl-v7.sv-en.lc.sv',\n",
    "              'data\\europarl-v7.sv-en.lc.en',\n",
    "              'data\\europarl-v7.de-en.lc.de',\n",
    "              'data\\europarl-v7.de-en.lc.en',\n",
    "              'data\\europarl-v7.fr-en.lc.fr',\n",
    "              'data\\europarl-v7.fr-en.lc.en']\n",
    "\n",
    "\n",
    "#Initialize the word counter \n",
    "eur_parl_frequency_without_symbols = word_frequency(file_paths[0], remove_symbols=True)\n",
    "\n",
    "\n",
    "for path in file_paths[1:]:\n",
    "\n",
    "    eur_parl_frequency_without_symbols += word_frequency(path, remove_symbols=True)\n",
    "\n",
    "\n",
    "eur_parl_frequency_without_symbols.most_common(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of speaker: 0.00217 %\n"
     ]
    }
   ],
   "source": [
    "words_amount_without_symbols = sum(eur_parl_frequency_without_symbols.values())\n",
    "\n",
    "speaker_probability_without_symbols = (eur_parl_frequency_without_symbols['speaker'] / words_amount_without_symbols ) * 100\n",
    "\n",
    "print(f'Probability of speaker: {speaker_probability_without_symbols:.5f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of the word *speaker* is slightly higher, which is reasonable with all the symbols removed. The word zebra had zero occurences in the original word frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we implement a bigram model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    '''\n",
    "    Reads the file, each sentence line by line and splits the words from the sentences. \n",
    "    Returns a list of lists with words from each sentence\n",
    "    '''\n",
    "    \n",
    "    sentences_list = []\n",
    "\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "    \n",
    "        for line in f:\n",
    "            \n",
    "            words = line.strip().split()\n",
    "            sentences_list.append(words)\n",
    "    \n",
    "    return sentences_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel:\n",
    "    def __init__(self):\n",
    "        self.bigram_counts = defaultdict(Counter)\n",
    "        self.starting_words = []\n",
    "\n",
    "    def train(self, sentence_list):\n",
    "        # Preprocess the text into words\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            self.starting_words.append(sentence[0])\n",
    "        \n",
    "        # Count bigrams in the text\n",
    "            for i in range(len(sentence) - 1):\n",
    "                self.bigram_counts[sentence[i]][sentence[i+1]] += 1\n",
    "        \n",
    "\n",
    "    def predict_next_word(self, word):\n",
    "        if word not in self.bigram_counts:\n",
    "            return None\n",
    "        next_words = self.bigram_counts[word]\n",
    "        total_counts = sum(next_words.values())\n",
    "        # Create a weighted choice among the next possible words\n",
    "        weighted_choices = [(w, count / total_counts) for w, count in next_words.items()]\n",
    "        return random.choices([w for w, _ in weighted_choices], [count for _, count in weighted_choices])[0]\n",
    "\n",
    "    def generate_text(self, start_word, length=10):\n",
    "\n",
    "        if start_word.lower() not in self.bigram_counts and not self.starting_words:\n",
    "            return \"Model not trained or start word not in corpus.\"\n",
    "        \n",
    "        if start_word.lower() in self.bigram_counts:\n",
    "            current_word = start_word.lower()\n",
    "\n",
    "        else:\n",
    "            current_word = random.choice(self.starting_words)\n",
    "        \n",
    "        \n",
    "        generated_text = [current_word]\n",
    "        \n",
    "        for _ in range(length - 1):\n",
    "            next_word = self.predict_next_word(current_word)\n",
    "            if next_word is None:\n",
    "                break  # End if no next word is found\n",
    "            generated_text.append(next_word)\n",
    "            current_word = next_word\n",
    "        return ' '.join(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you the matter of lead to thank you are the\n"
     ]
    }
   ],
   "source": [
    "eng_sentences = read_file(eng_swe_file_path)\n",
    "\n",
    "model = BigramModel()\n",
    "model.train(eng_sentences)\n",
    "\n",
    "start_word = \"You\"\n",
    "generated_text = model.generate_text(start_word, 10)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i believe it outlines the agenda ? will do . this . this has been almost four conclusions that parliament , especially regarding the european union , with this , mainly towards cultural and i believe there is a sustainable energy transmission rather they begin implementing it and , growth , ladies and therefore of action against europeanist dictatorship know , we should react against money from 1995 , not adding a functioning of demand-orientated economic conversion of every area . there is so these parameters of food in paris or underwater tunnels in the eu from the conditions and\n"
     ]
    }
   ],
   "source": [
    "start_word = \"\"\n",
    "generated_text = model.generate_text(start_word, 100)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the starting word does not appear in the bigrams, the model chooses a random starting word instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Translation modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented a translation model based on the pseudo code for the IBM model 1. The main component is the `train` method where the EM-algorithm is implemented with count collecting and probability updates. We also created some other methods that loads the sentence pairs, initializes translation probabilities, and finds the top translations for a word. In the initializing process, all the probabilities are uniform rather than random. \n",
    "\n",
    "\n",
    "During the translation generation phase, the model uses the learned probabilities P(f∣e) to generate English translations by selecting the English words that are most likely to translate into the observed foreign words. This is achieved through the decode method later that search for the best English sentence given the foreign sentence, using the probabilities learned by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the conditional probability $P(f∣e)$, the objective of the statistical translation model is to find the most probable translation of a foreign sentence into English. By using this probaility, we are essentially asking, \"given the English word $e$, what is the probability of each foreign word $f$ being its correct translation?\"\n",
    "\n",
    "The IBM model used simplifies translation by focusing on word alignments from English to the foreign language, and do not consider word order or syntax. The model thus learns to align and translate based on word occurrence probabilities, which are easier to estimate and optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationModel:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.translation_probabilities = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n",
    "        self.languages = []\n",
    "\n",
    "\n",
    "    def add_sentence_pairs(self, foreign_file_path, english_file_path, language): #add source_language='en'?\n",
    "        \n",
    "        if language in self.languages:\n",
    "            print(f'Language \\\"{language}\\\" is already implemented.')\n",
    "            return\n",
    "\n",
    "        self.languages.append(language)\n",
    "        \n",
    "        sentence_pairs = []\n",
    "        \n",
    "        with open(foreign_file_path, 'r') as foreign_file, \\\n",
    "             open(english_file_path, 'r') as english_file:\n",
    "            \n",
    "            for foreign_sentence, english_sentence in zip(foreign_file, english_file):\n",
    "                \n",
    "                foreign_sentence = foreign_sentence.split()\n",
    "                english_sentence = english_sentence.split() + ['NULL'] #idk if this works correctly\n",
    "                \n",
    "                sentence_pairs.append((foreign_sentence, english_sentence))\n",
    "\n",
    "        self.train(sentence_pairs, language)\n",
    "\n",
    "\n",
    "    def train(self, sentence_pairs, language, iterations=10):\n",
    "        \n",
    "        translation_probabilities = self.initialize_translation_probabilities(sentence_pairs)\n",
    "        \n",
    "        #Expectation-maximization algoritm\n",
    "        for iteration in range(iterations):\n",
    "            \n",
    "            counts_fe = defaultdict(lambda: defaultdict(float))\n",
    "            counts_e = defaultdict(float)\n",
    "            total_f = defaultdict(float)\n",
    "            \n",
    "            #collect counts\n",
    "            for (foreign_sentence, english_sentence) in sentence_pairs:\n",
    "\n",
    "                for f in foreign_sentence:\n",
    "                    \n",
    "                    total_f[f] = sum(translation_probabilities[e][f] for e in english_sentence)\n",
    "\n",
    "                for e in english_sentence:\n",
    "                    for f in foreign_sentence:\n",
    "                        \n",
    "                        count = translation_probabilities[e][f] / total_f[f]\n",
    "                        counts_fe[e][f] += count\n",
    "                        counts_e[e] += count\n",
    "            \n",
    "            #update probabilities\n",
    "            for e in counts_fe:\n",
    "                for f in counts_fe[e]:\n",
    "                    \n",
    "                    translation_probabilities[e][f] = counts_fe[e][f] / counts_e[e]\n",
    "        \n",
    "        self.translation_probabilities[language] = translation_probabilities\n",
    "\n",
    "\n",
    "    def initialize_translation_probabilities(self, sentence_pairs):\n",
    "\n",
    "        translation_probabilities = defaultdict(lambda: defaultdict(float))\n",
    "        \n",
    "        for (foreign_sentence, english_sentence) in sentence_pairs:\n",
    "            \n",
    "            uniform_probability = 1 / len(english_sentence)\n",
    "            \n",
    "            for f in foreign_sentence:\n",
    "                for e in english_sentence:\n",
    "                    \n",
    "                    translation_probabilities[e][f] = uniform_probability\n",
    "        \n",
    "        return translation_probabilities\n",
    "\n",
    "\n",
    "    def find_top_translations(self, word, language, n=10):\n",
    "        \n",
    "        if language not in self.languages:\n",
    "            \n",
    "            print(f'Language \\\"{language}\\\" not found.')\n",
    "            return\n",
    "       \n",
    "        translations = [(f, prob) for f, prob in self.translation_probabilities[language][word].items()]\n",
    "        translations.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if len(translations) == 0:\n",
    "            \n",
    "            print(f'Translation of \\\"{word}\\\" not found.')\n",
    "            return\n",
    "\n",
    "        return translations[:n]\n",
    "    \n",
    "\n",
    "    def decode(self, foreign_sentence, language):\n",
    "\n",
    "        english_translation = []\n",
    "\n",
    "        for f in foreign_sentence.split():\n",
    "\n",
    "            possible_translations = {e: self.translation_probabilities[language][e].get(f, 0) for e in self.translation_probabilities[language]}\n",
    "            \n",
    "            e = max(possible_translations, key=possible_translations.get)\n",
    "            \n",
    "            english_translation.append(e)\n",
    "        \n",
    "        return ' '.join(english_translation)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_model = TranslationModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_model.add_sentence_pairs(swe_eng_file_path, eng_swe_file_path, 'se')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('europeiska', 0.8358811726335589),\n",
       " ('europeisk', 0.07793251129315405),\n",
       " ('den', 0.01234342827816025),\n",
       " ('i', 0.01145802549009602),\n",
       " ('att', 0.0073175106656019956),\n",
       " ('en', 0.0064164076171784445),\n",
       " ('till', 0.006403088625106279),\n",
       " ('det', 0.005713358673491642),\n",
       " (',', 0.005560543111116395),\n",
       " ('för', 0.0047190385350298)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_model.find_top_translations('european', 'se')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_model.add_sentence_pairs(ger_eng_file_path, eng_ger_file_path, 'de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('europäischen', 0.6583104427978661),\n",
       " ('europäische', 0.2994256203430025),\n",
       " ('der', 0.015614335052903423),\n",
       " ('die', 0.006707869728434155),\n",
       " (',', 0.0046853348392669105),\n",
       " ('den', 0.002090016898065694),\n",
       " ('in', 0.002076244515776026),\n",
       " ('.', 0.001742672454766282),\n",
       " ('union', 0.0017056204984250926),\n",
       " ('das', 0.0015701739731939042)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_model.find_top_translations('european', 'de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_model.add_sentence_pairs(fre_eng_file_path, eng_fre_file_path, 'fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('européenne', 0.46255524097583534),\n",
       " ('européen', 0.2740681214755836),\n",
       " ('l', 0.07615189999366892),\n",
       " ('&apos;', 0.06673961686785357),\n",
       " ('de', 0.043416375856401186),\n",
       " (',', 0.012650462639585984),\n",
       " ('le', 0.011782554822424903),\n",
       " ('la', 0.008875011271880097),\n",
       " ('au', 0.00688588253565706),\n",
       " ('d', 0.005003206485329585)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_model.find_top_translations('european', 'fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding is challenging because we have a very large search space. From the outputs above, we see that for each foreign word there are multiple possible translations in English. In the IBM model 1 word alignments are assumed to be indepent of each other so the order of thr words and context is not considered which leads to less natural sentence translations. \n",
    "\n",
    "\n",
    "Since our model is trained with the translation probabilities $P(f|e)$ (the probability of a foreign word \n",
    "$f$ given an English word $e$), we can not simply find the English word with the highest translation probability $P(e|f)$. A solution would be to also compute the \"reverse\" translation probabilities $P(e|f) in the model. However, in our decoder, we instead use the trained model in a reverse lookup manner by using the existing translation probabilities but select the English word that maximizes the probability for the foreign word. \n",
    "\n",
    "\n",
    "We do this for each word, and all these words are then combined into the output sentence. This means that the the model does not consider word order or grammatical structure by looking at the neighboring words. This simpliciation impacts the translation quality because the a word with the highest probability will not always be the word that fits the context of the sentence, which leads to a translation that is less fluent or grammatically correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i is one politicians'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'jag är en politiker'\n",
    "\n",
    "translation_model.decode(sentence, 'se')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example sentence above, the Swedish sentence *Jag är en politiker* should in reality translate to *I am a politician* in English. The decoder output conveys the concept but is obviously not grammatically correct. This is a result of the implementation of the decoder which uses the translated word with highest probability. The word *politiker* could be both singular and plural in Swedish and the translation *politician* has a higher probability as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politician: ('politiker', 0.16913921184700276)\n",
      "Politicians: ('politiker', 0.4493980991701494)\n"
     ]
    }
   ],
   "source": [
    "print('Politician:', translation_model.find_top_translations('politician', 'se')[0])\n",
    "print('Politicians:', translation_model.find_top_translations('politicians', 'se')[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "For a translation to be good, it should be as understandable and as accurate as possible. Sometimes direct translations do not exist, and in that case it is more important to convey the same message as the sentence is trying to say, instead of attempting a direct translation. It can also be useful to simplify sentences, to avoid misunderstandings and increase the likelihood that the translation is done correctly.\n",
    "\n",
    "Evaluating translations can be done manually or automatically. In the case of manual evaluation, simply having a person who is fluent in both languages would be a great asset, as this person would be able to properly understand both sentences and evaluate how well the sentences are translated, and perhaps even come up with better translation suggestions. The advantage of this is that our evaluation will be done very accurately, however, this is very time costly and expensive. It is simply unfeasible to have people evaluate every single sentence translation. In order to automatically translate sentences, we can use a set of test data, where we have a sentence in the source language and a respective sentence in the language the sentence is translated into. We then evaluate the translated sentences on this test data, where we have a “correct” translation. These sentences would preferably not be used in the training data to get a good evaluation. It would however still be interesting to evaluate the translation system on sentences in the training data, as sometimes the translation system could get confused. But this has the limitation of not being able to be as accurate as a human fluent in both languages. Some times sentences can be translated in different ways, and the same message will still get across, and there could be many ways of saying the same thing, and it would still be a correct translation. This is difficult to consider when using automatic evaluation as it is difficult to consider all variants and translations when evaluating.\n",
    "\n",
    "When talking about translations in 2024 it is also important to consider LLM:s, as these are very good at “understanding” what is being said, and can often translate the message into other languages very well. If your translation system is not already LLM based, using these for translation evaluation could be don. For example; a model could be asked if the message is properly conveyed, or if the sentence is properly translated, and output the information in any code or output format asked. However, these models also vary in quality, speed and cost. They can be a bit random sometimes.\n",
    "\n",
    "### b)\n",
    "\n",
    "Although we are not experts in the Estonian language we could argue that it is both a feature and a bug. It depends on what we prioritize in our translation. Perhaps it is difficult to translate into different genders in the Estonian language or gender changes how the noun is written? In that case the model may be correct in its translation more times if it simply uses a gender neutral form. However, this may cause the translation to sometimes not convey the message properly. If a text is talking about a male and a female, and it refers to either of them simply using a he and she, the gender neutral translation would confuse the reader as to which person is mentioned. \n",
    "\n",
    "Another interesting point is that there could have been a bias in the training dataset for the translation model. Historically, medicine and technology was predominantly practiced by men even though the fields have become more diverse today. This by itself may lead to a bias in the translation model favoring a specific gender for the translation depending on the context. A simple solution to this issue is to use better and inclusive data, although this may prove to be a challange by itself.\n",
    "\n",
    "### c)\n",
    "\n",
    "In the first example, the sentence has a clear context where the ball is being hit with the bat, which gives a bit of context that it is a baseball bat and not the animal. The second sentence also gives clear context saying that this bat eats insects, giving a clear indication that we are talking about the animal. However in the last sentence, the translation gets confused. We think what particularly confuses it is the use of the \"forest\" in the text. As the word trä in slagträ means tree, and trees are in the forest. So using trä makes sense, as in slagträ, but a slagträ doesn’t live anywhere, that does not make sense either. Instead it comes up with this word combination (combining words is normal in swedish) but makes up a word that doesn’t exist so that it makes sense for both a tree in the forest, and an animal bat living.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
