{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Module 5: Natural language processing**\n",
    "## DAT410\n",
    "\n",
    "### Group 29 \n",
    "### David Laessker, 980511-5012, laessker@chalmers.se\n",
    "\n",
    "### Oskar Palmgren, 010529-4714, oskarpal@chalmers.se\n",
    "\n",
    "\n",
    "\n",
    "We hereby declare that we have both actively participated in solving every exercise. All solutions are entirely our own work, without having taken part of other solutions.\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Reading and reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Like speech recognition and image recognition?\n",
    "\n",
    "b) Systems that are rule-based explicitly use linguistic rules and dictionaries, while neural systems learn these linguistic patterns from large datasets. Both approaches aim to accurately translate languages by mapping structures and meanings, but through different means.\n",
    "\n",
    "c) Maybe smaller datasets? Modern neural systems may not capture the grammatical patterns in the language with scarce data. A rule based system will therefore offer more predictable and interpretable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "swe_eng_file_path = 'data/europarl-v7.sv-en.lc.sv'\n",
    "eng_swe_file_path = 'data\\europarl-v7.sv-en.lc.en'\n",
    "\n",
    "ger_eng_file_path = 'data\\europarl-v7.de-en.lc.de'\n",
    "eng_ger_file_path = 'data\\europarl-v7.de-en.lc.en'\n",
    "\n",
    "fre_eng_file_path = 'data\\europarl-v7.fr-en.lc.en'\n",
    "eng_fre_file_path = 'data\\europarl-v7.fr-en.lc.fr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency(file):\n",
    "    \n",
    "    word_counter = Counter()\n",
    "\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "    \n",
    "        for line in f:\n",
    "            \n",
    "            words = line.strip().split()\n",
    "            word_counter.update(words)\n",
    "    \n",
    "    return word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 9648),\n",
       " ('att', 9181),\n",
       " (',', 8876),\n",
       " ('och', 7038),\n",
       " ('i', 5949),\n",
       " ('det', 5687),\n",
       " ('som', 5028),\n",
       " ('för', 4959),\n",
       " ('av', 4013),\n",
       " ('är', 3840)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swe_frequency = word_frequency(swe_eng_file_path)\n",
    "\n",
    "swe_top_10 = swe_frequency.most_common(10)\n",
    "\n",
    "swe_top_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 19322),\n",
       " (',', 13514),\n",
       " ('.', 9774),\n",
       " ('of', 9312),\n",
       " ('to', 8801),\n",
       " ('and', 6946),\n",
       " ('in', 6090),\n",
       " ('is', 4400),\n",
       " ('that', 4357),\n",
       " ('a', 4269)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_frequency = word_frequency(eng_swe_file_path) #maybe we should combine all the english files?\n",
    "\n",
    "eng_top_10 = eng_frequency.most_common(10)\n",
    "\n",
    "eng_top_10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to remove punctuations, and maybe stop words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281476\n",
      "Probability of speaker: 0.000036\n",
      "Probability of zebra: 0.0\n"
     ]
    }
   ],
   "source": [
    "eng_words_amount = sum(eng_frequency.values())\n",
    "\n",
    "#print(eng_words_amount)\n",
    "#print(eng_frequency['speaker'] )\n",
    "#print(eng_frequency['zebra'] )\n",
    "\n",
    "speaker_probability = eng_frequency['speaker'] / eng_words_amount\n",
    "zebra_probability = eng_frequency['zebra'] / eng_words_amount\n",
    "\n",
    "# calculate probability for \"speaker\" and \"zebra\" in the english text, but we need to look at all the languages?\n",
    "print(f'Probability of speaker: {speaker_probability:.6f}')\n",
    "print(f'Probability of zebra: {zebra_probability}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['som',\n",
       " 'ni',\n",
       " 'kunnat',\n",
       " 'konstatera',\n",
       " 'ägde',\n",
       " '&quot;',\n",
       " 'den',\n",
       " 'stora',\n",
       " 'år',\n",
       " '2000-buggen',\n",
       " '&quot;',\n",
       " 'aldrig',\n",
       " 'rum',\n",
       " '.',\n",
       " 'däremot',\n",
       " 'har',\n",
       " 'invånarna',\n",
       " 'i',\n",
       " 'ett',\n",
       " 'antal',\n",
       " 'av',\n",
       " 'våra',\n",
       " 'medlemsländer',\n",
       " 'drabbats',\n",
       " 'av',\n",
       " 'naturkatastrofer',\n",
       " 'som',\n",
       " 'verkligen',\n",
       " 'varit',\n",
       " 'förskräckliga',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_list = []\n",
    "\n",
    "with open(swe_eng_file_path, 'r', encoding='utf-8') as f:\n",
    "    \n",
    "        for line in f:\n",
    "            \n",
    "            words = line.strip().split()\n",
    "            words_list.append(words)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class BigramModel:\n",
    "    def __init__(self):\n",
    "        self.bigram_counts = defaultdict(Counter)\n",
    "        self.starting_words = []\n",
    "\n",
    "    def train(self, sentence_list):\n",
    "        # Preprocess the text into words\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            self.starting_words.append(sentence[0])\n",
    "        \n",
    "        # Count bigrams in the text\n",
    "            for i in range(len(sentence) - 1):\n",
    "                self.bigram_counts[sentence[i]][sentence[i+1]] += 1\n",
    "        \n",
    "\n",
    "    def predict_next_word(self, word):\n",
    "        if word not in self.bigram_counts:\n",
    "            return None\n",
    "        next_words = self.bigram_counts[word]\n",
    "        total_counts = sum(next_words.values())\n",
    "        # Create a weighted choice among the next possible words\n",
    "        weighted_choices = [(w, count / total_counts) for w, count in next_words.items()]\n",
    "        return random.choices([w for w, _ in weighted_choices], [count for _, count in weighted_choices])[0]\n",
    "\n",
    "    def generate_text(self, start_word, length=10):\n",
    "        \n",
    "        #print(self.starting_words)\n",
    "        #print(self.bigram_counts['jag'])\n",
    "\n",
    "\n",
    "        if start_word.lower() not in self.bigram_counts and not self.starting_words:\n",
    "            return \"Model not trained or start word not in corpus.\"\n",
    "        \n",
    "        if start_word.lower() in self.bigram_counts:\n",
    "            current_word = start_word.lower()\n",
    "\n",
    "        else:\n",
    "            current_word = random.choice(self.starting_words)\n",
    "        \n",
    "        \n",
    "        generated_text = [current_word]\n",
    "        \n",
    "        for _ in range(length - 1):\n",
    "            next_word = self.predict_next_word(current_word)\n",
    "            if next_word is None:\n",
    "                break  # End if no next word is found\n",
    "            generated_text.append(next_word)\n",
    "            current_word = next_word\n",
    "        return ' '.join(generated_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jag trodde att hjälpa människor som vi beklagar hans betänkande om dessa medel att godta ändringsförslag faktiskt inte att efterfrågan för att den europeiska unionen , när en allvarlig begränsning av roth-behrendt sade , och de människor måste samtidigt de summor av personer , är det irländska myndigheterna , för att man är den också viktigt som vi får inte någon stadga över vad det var lönsamt att kväva teatergrupper , deltar i strid med hur brådskande ärende , och kommissionen gjort sig därför har utsatts för någon betoning på att visa att den risken för kvestorernas möte där nationella\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "model = BigramModel()\n",
    "model.train(words_list)\n",
    "\n",
    "start_word = \"jag\"\n",
    "generated_text = model.generate_text(start_word, 100)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def createBigram(data):\n",
    "   listOfBigrams = []\n",
    "   bigramCounts = {}\n",
    "   unigramCounts = {}\n",
    "   for i in range(len(data)-1):\n",
    "      if i < len(data) - 1 and data[i+1].islower():\n",
    "\n",
    "         listOfBigrams.append((data[i], data[i + 1]))\n",
    "\n",
    "         if (data[i], data[i+1]) in bigramCounts:\n",
    "            bigramCounts[(data[i], data[i + 1])] += 1\n",
    "         else:\n",
    "            bigramCounts[(data[i], data[i + 1])] = 1\n",
    "\n",
    "      if data[i] in unigramCounts:\n",
    "         unigramCounts[data[i]] += 1\n",
    "      else:\n",
    "         unigramCounts[data[i]] = 1\n",
    "   return listOfBigrams, unigramCounts, bigramCounts\n",
    "\n",
    "\n",
    "def calcBigramProb(listOfBigrams, unigramCounts, bigramCounts):\n",
    "    listOfProb = {}\n",
    "    for bigram in listOfBigrams:\n",
    "        word1 = bigram[0]\n",
    "        word2 = bigram[1]\n",
    "        listOfProb[bigram] = (bigramCounts.get(bigram))/(unigramCounts.get(word1))\n",
    "    return listOfProb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'islower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m listOfBigrams, unigramCounts, bigramCounts \u001b[38;5;241m=\u001b[39m \u001b[43mcreateBigram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m All the possible Bigrams are \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(listOfBigrams)\n",
      "Cell \u001b[1;32mIn[78], line 6\u001b[0m, in \u001b[0;36mcreateBigram\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      4\u001b[0m unigramCounts \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 6\u001b[0m    \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mislower\u001b[49m():\n\u001b[0;32m      8\u001b[0m       listOfBigrams\u001b[38;5;241m.\u001b[39mappend((data[i], data[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m     10\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m (data[i], data[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;129;01min\u001b[39;00m bigramCounts:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'islower'"
     ]
    }
   ],
   "source": [
    "listOfBigrams, unigramCounts, bigramCounts = createBigram(words_list[0])\n",
    "\n",
    "print(\"\\n All the possible Bigrams are \")\n",
    "print(listOfBigrams)\n",
    "\n",
    "print(\"\\n Bigrams along with their frequency \")\n",
    "print(bigramCounts)\n",
    "\n",
    "print(\"\\n Unigrams along with their frequency \")\n",
    "print(unigramCounts)\n",
    "\n",
    "bigramProb = calcBigramProb(listOfBigrams, unigramCounts, bigramCounts)\n",
    "\n",
    "print(\"\\n Bigrams along with their probability \")\n",
    "print(bigramProb)\n",
    "inputList=\"This is my cat\"\n",
    "splt=inputList.split()\n",
    "outputProb1 = 1\n",
    "bilist=[]\n",
    "bigrm=[]\n",
    "\n",
    "for i in range(len(splt) - 1):\n",
    "    if i < len(splt) - 1:\n",
    "\n",
    "        bilist.append((splt[i], splt[i + 1]))\n",
    "\n",
    "print(\"\\n The bigrams in given sentence are \")\n",
    "print(bilist)\n",
    "for i in range(len(bilist)):\n",
    "    if bilist[i] in bigramProb:\n",
    "\n",
    "        outputProb1 *= bigramProb[bilist[i]]\n",
    "    else:\n",
    "\n",
    "        outputProb1 *= 0\n",
    "print('\\n' + 'Probablility of sentence \\\"This is my cat\\\" = ' + str(outputProb1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
