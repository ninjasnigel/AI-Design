{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Module 5: Natural language processing**\n",
    "## DAT410\n",
    "\n",
    "### Group 29 \n",
    "### David Laessker, 980511-5012, laessker@chalmers.se\n",
    "\n",
    "### Oskar Palmgren, 010529-4714, oskarpal@chalmers.se\n",
    "\n",
    "\n",
    "\n",
    "We hereby declare that we have both actively participated in solving every exercise. All solutions are entirely our own work, without having taken part of other solutions.\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Reading and reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Like speech recognition and image recognition?\n",
    "\n",
    "b) Systems that are rule-based explicitly use linguistic rules and dictionaries, while neural systems learn these linguistic patterns from large datasets. Both approaches aim to accurately translate languages by mapping structures and meanings, but through different means.\n",
    "\n",
    "c) Maybe smaller datasets? Modern neural systems may not capture the grammatical patterns in the language with scarce data. A rule based system will therefore offer more predictable and interpretable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from html import unescape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "swe_eng_file_path = 'data/europarl-v7.sv-en.lc.sv'\n",
    "eng_swe_file_path = 'data\\europarl-v7.sv-en.lc.en'\n",
    "\n",
    "ger_eng_file_path = 'data\\europarl-v7.de-en.lc.de'\n",
    "eng_ger_file_path = 'data\\europarl-v7.de-en.lc.en'\n",
    "\n",
    "fre_eng_file_path = 'data\\europarl-v7.fr-en.lc.fr'\n",
    "eng_fre_file_path = 'data\\europarl-v7.fr-en.lc.en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a function that\n",
    "\n",
    "\n",
    "We noticed that punctuations (periods and commas) were in the top 10 in all languages. French also had HTML entity \"&apos\" (apostrohe), so we also added an agrument to be able to calculate the word frequency without special symbols. However, this is only used in this task because for these symbols may be important and handles differently in the translation model, for example punctuations for sentence boundary detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency(file, remove_symbols=False):\n",
    "    \n",
    "    word_counter = Counter()\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "    \n",
    "        for line in f:\n",
    "\n",
    "            if remove_symbols:\n",
    "                line = unescape(line)\n",
    "                line = re.sub(r'[^\\w\\s]', '', line)\n",
    "            \n",
    "            words = line.split()\n",
    "            word_counter.update(words)\n",
    "    \n",
    "    return word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 9648),\n",
       " ('att', 9181),\n",
       " (',', 8876),\n",
       " ('och', 7038),\n",
       " ('i', 5949),\n",
       " ('det', 5687),\n",
       " ('som', 5028),\n",
       " ('för', 4959),\n",
       " ('av', 4013),\n",
       " ('är', 3840)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swe_frequency = word_frequency(swe_eng_file_path)\n",
    "\n",
    "swe_top_10 = swe_frequency.most_common(10)\n",
    "\n",
    "swe_top_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 58790),\n",
       " (',', 42043),\n",
       " ('.', 29542),\n",
       " ('of', 28406),\n",
       " ('to', 26842),\n",
       " ('and', 21459),\n",
       " ('in', 18485),\n",
       " ('is', 13331),\n",
       " ('that', 13219),\n",
       " ('a', 13090)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_frequency1 = word_frequency(eng_swe_file_path)\n",
    "eng_frequency2 = word_frequency(eng_ger_file_path)\n",
    "eng_frequency3 = word_frequency(eng_fre_file_path)\n",
    "\n",
    "eng_total_frequency = eng_frequency1 + eng_frequency2 + eng_frequency3\n",
    "\n",
    "eng_top_10 = eng_total_frequency.most_common(10)\n",
    "\n",
    "eng_top_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 18549),\n",
       " ('die', 10521),\n",
       " ('.', 9733),\n",
       " ('der', 9374),\n",
       " ('und', 7028),\n",
       " ('in', 4175),\n",
       " ('zu', 3168),\n",
       " ('den', 2976),\n",
       " ('wir', 2863),\n",
       " ('daß', 2738)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ger_frequency = word_frequency(ger_eng_file_path)\n",
    "\n",
    "ger_top_10 = ger_frequency.most_common(10)\n",
    "\n",
    "ger_top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('&apos;', 16729),\n",
       " (',', 15402),\n",
       " ('de', 14520),\n",
       " ('la', 9746),\n",
       " ('.', 9734),\n",
       " ('et', 6619),\n",
       " ('l', 6536),\n",
       " ('le', 6174),\n",
       " ('les', 5585),\n",
       " ('à', 5500)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fre_frequency = word_frequency(fre_eng_file_path)\n",
    "\n",
    "fre_top_10 = fre_frequency.most_common(10)\n",
    "\n",
    "fre_top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 84870),\n",
       " ('the', 58812),\n",
       " ('.', 58657),\n",
       " ('of', 28414),\n",
       " ('to', 26843),\n",
       " ('in', 22808),\n",
       " ('and', 21463),\n",
       " ('&apos;', 20013),\n",
       " ('de', 17536),\n",
       " ('i', 14894)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eur_parl_frequency = swe_frequency + fre_frequency + ger_frequency + eng_total_frequency\n",
    "\n",
    "eur_parl_top_10 = eur_parl_frequency.most_common(10)\n",
    "\n",
    "eur_parl_top_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of speaker: 0.00193 %\n",
      "Probability of zebra: 0.0 %\n"
     ]
    }
   ],
   "source": [
    "words_amount = sum(eur_parl_frequency.values())\n",
    "\n",
    "#print(words_amount)\n",
    "#print(eur_parl_frequency['speaker'])\n",
    "#print(eur_parl_frequency['zebra'])\n",
    "\n",
    "speaker_probability = (eur_parl_frequency['speaker'] / words_amount ) * 100\n",
    "zebra_probability = (eur_parl_frequency['zebra'] / words_amount ) * 100\n",
    "\n",
    "# calculate probability for \"speaker\" and \"zebra\"\n",
    "print(f'Probability of speaker: {speaker_probability:.5f} %')\n",
    "print(f'Probability of zebra: {zebra_probability} %')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now without symbols.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 58812),\n",
       " ('of', 28414),\n",
       " ('to', 26843),\n",
       " ('in', 22808),\n",
       " ('and', 21463),\n",
       " ('de', 17536),\n",
       " ('i', 14907),\n",
       " ('a', 14853),\n",
       " ('is', 13340),\n",
       " ('that', 13219)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Without symbols\n",
    "\n",
    "file_paths = ['data/europarl-v7.sv-en.lc.sv',\n",
    "              'data\\europarl-v7.sv-en.lc.en',\n",
    "              'data\\europarl-v7.de-en.lc.de',\n",
    "              'data\\europarl-v7.de-en.lc.en',\n",
    "              'data\\europarl-v7.fr-en.lc.fr',\n",
    "              'data\\europarl-v7.fr-en.lc.en']\n",
    "\n",
    "\n",
    "#Initialize the word counter \n",
    "eur_parl_frequency_without_symbols = word_frequency(file_paths[0], remove_symbols=True)\n",
    "\n",
    "\n",
    "for path in file_paths[1:]:\n",
    "\n",
    "    eur_parl_frequency_without_symbols += word_frequency(path, remove_symbols=True)\n",
    "\n",
    "\n",
    "eur_parl_frequency_without_symbols.most_common(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of speaker: 0.00217 %\n"
     ]
    }
   ],
   "source": [
    "words_amount_without_symbols = sum(eur_parl_frequency_without_symbols.values())\n",
    "\n",
    "speaker_probability_without_symbols = (eur_parl_frequency_without_symbols['speaker'] / words_amount_without_symbols ) * 100\n",
    "\n",
    "print(f'Probability of speaker: {speaker_probability_without_symbols:.5f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "0\n",
      "0\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "#Perhaps an issue: merges words that consists of hyphens\n",
    "\n",
    "print(eur_parl_frequency['vice-president'])\n",
    "print(eur_parl_frequency['vicepresident'])\n",
    "\n",
    "print(eur_parl_frequency_without_symbols['vice-president'])\n",
    "print(eur_parl_frequency_without_symbols['vicepresident'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    '''\n",
    "    Reads the file, each sentence line by line and splits the words from the sentences. \n",
    "    Returns a list of lists with words from each sentence\n",
    "    '''\n",
    "    \n",
    "    sentences_list = []\n",
    "\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "    \n",
    "        for line in f:\n",
    "            \n",
    "            words = line.strip().split()\n",
    "            sentences_list.append(words)\n",
    "    \n",
    "    return sentences_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel:\n",
    "    def __init__(self):\n",
    "        self.bigram_counts = defaultdict(Counter)\n",
    "        self.starting_words = []\n",
    "\n",
    "    def train(self, sentence_list):\n",
    "        # Preprocess the text into words\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            self.starting_words.append(sentence[0])\n",
    "        \n",
    "        # Count bigrams in the text\n",
    "            for i in range(len(sentence) - 1):\n",
    "                self.bigram_counts[sentence[i]][sentence[i+1]] += 1\n",
    "        \n",
    "\n",
    "    def predict_next_word(self, word):\n",
    "        if word not in self.bigram_counts:\n",
    "            return None\n",
    "        next_words = self.bigram_counts[word]\n",
    "        total_counts = sum(next_words.values())\n",
    "        # Create a weighted choice among the next possible words\n",
    "        weighted_choices = [(w, count / total_counts) for w, count in next_words.items()]\n",
    "        return random.choices([w for w, _ in weighted_choices], [count for _, count in weighted_choices])[0]\n",
    "\n",
    "    def generate_text(self, start_word, length=10):\n",
    "        \n",
    "        #print(self.starting_words)\n",
    "        #print(self.bigram_counts['jag'])\n",
    "\n",
    "\n",
    "        if start_word.lower() not in self.bigram_counts and not self.starting_words:\n",
    "            return \"Model not trained or start word not in corpus.\"\n",
    "        \n",
    "        if start_word.lower() in self.bigram_counts:\n",
    "            current_word = start_word.lower()\n",
    "\n",
    "        else:\n",
    "            current_word = random.choice(self.starting_words)\n",
    "        \n",
    "        \n",
    "        generated_text = [current_word]\n",
    "        \n",
    "        for _ in range(length - 1):\n",
    "            next_word = self.predict_next_word(current_word)\n",
    "            if next_word is None:\n",
    "                break  # End if no next word is found\n",
    "            generated_text.append(next_word)\n",
    "            current_word = next_word\n",
    "        return ' '.join(generated_text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jag , att gå igenom . ( h-0041 / 1999\n"
     ]
    }
   ],
   "source": [
    "swe_sentences = read_file(swe_eng_file_path)\n",
    "\n",
    "model = BigramModel()\n",
    "model.train(swe_sentences)\n",
    "\n",
    "start_word = \"jag\"\n",
    "generated_text = model.generate_text(start_word, 10)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Need to remove punctuations here too?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndef createBigram(data):\\n   listOfBigrams = []\\n   bigramCounts = {}\\n   unigramCounts = {}\\n   for sentence in data:\\n      for i in range(len(sentence)-1):\\n         if i < len(sentence) - 1 and sentence[i+1].islower():\\n\\n            listOfBigrams.append((sentence[i], sentence[i + 1]))\\n\\n            if (sentence[i], sentence[i+1]) in bigramCounts:\\n               bigramCounts[(sentence[i], sentence[i + 1])] += 1\\n            else:\\n               bigramCounts[(sentence[i], sentence[i + 1])] = 1\\n\\n         if sentence[i] in unigramCounts:\\n            unigramCounts[sentence[i]] += 1\\n         else:\\n            unigramCounts[sentence[i]] = 1\\n   return listOfBigrams, unigramCounts, bigramCounts\\n\\n\\ndef calcBigramProb(listOfBigrams, unigramCounts, bigramCounts):\\n    listOfProb = {}\\n    for bigram in listOfBigrams:\\n        word1 = bigram[0]\\n        word2 = bigram[1]\\n        listOfProb[bigram] = (bigramCounts.get(bigram))/(unigramCounts.get(word1))\\n    return listOfProb\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "def createBigram(data):\n",
    "   listOfBigrams = []\n",
    "   bigramCounts = {}\n",
    "   unigramCounts = {}\n",
    "   for sentence in data:\n",
    "      for i in range(len(sentence)-1):\n",
    "         if i < len(sentence) - 1 and sentence[i+1].islower():\n",
    "\n",
    "            listOfBigrams.append((sentence[i], sentence[i + 1]))\n",
    "\n",
    "            if (sentence[i], sentence[i+1]) in bigramCounts:\n",
    "               bigramCounts[(sentence[i], sentence[i + 1])] += 1\n",
    "            else:\n",
    "               bigramCounts[(sentence[i], sentence[i + 1])] = 1\n",
    "\n",
    "         if sentence[i] in unigramCounts:\n",
    "            unigramCounts[sentence[i]] += 1\n",
    "         else:\n",
    "            unigramCounts[sentence[i]] = 1\n",
    "   return listOfBigrams, unigramCounts, bigramCounts\n",
    "\n",
    "\n",
    "def calcBigramProb(listOfBigrams, unigramCounts, bigramCounts):\n",
    "    listOfProb = {}\n",
    "    for bigram in listOfBigrams:\n",
    "        word1 = bigram[0]\n",
    "        word2 = bigram[1]\n",
    "        listOfProb[bigram] = (bigramCounts.get(bigram))/(unigramCounts.get(word1))\n",
    "    return listOfProb\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Translation modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the conditional probability $P(f∣e)$, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationModel:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.translation_probabilities = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n",
    "        self.languages = []\n",
    "\n",
    "\n",
    "    def add_sentence_pairs(self, foreign_file_path, english_file_path, language): #add source_language='en'?\n",
    "        \n",
    "        if language in self.languages:\n",
    "            print(f'Language \\\"{language}\\\" is already implemented.')\n",
    "            return\n",
    "\n",
    "        #if language not in self.languages:\n",
    "        self.languages.append(language)\n",
    "        \n",
    "        sentence_pairs = []\n",
    "        \n",
    "        with open(foreign_file_path, 'r') as foreign_file, \\\n",
    "             open(english_file_path, 'r') as english_file:\n",
    "            \n",
    "            for foreign_sentence, english_sentence in zip(foreign_file, english_file):\n",
    "                \n",
    "                foreign_sentence = foreign_sentence.split()\n",
    "                english_sentence = english_sentence.split() + ['NULL'] #idk if this works correctly\n",
    "                \n",
    "                sentence_pairs.append((foreign_sentence, english_sentence))\n",
    "\n",
    "        self.train(sentence_pairs, language)\n",
    "\n",
    "\n",
    "    def train(self, sentence_pairs, language, iterations=10):\n",
    "        \n",
    "        translation_probabilities = self.initialize_translation_probabilities(sentence_pairs)\n",
    "        \n",
    "        #Expectation-maximization algoritm\n",
    "        for iteration in range(iterations):\n",
    "            \n",
    "            counts_fe = defaultdict(lambda: defaultdict(float))\n",
    "            counts_e = defaultdict(float)\n",
    "            total_f = defaultdict(float)\n",
    "            \n",
    "            #collect counts\n",
    "            for (foreign_sentence, english_sentence) in sentence_pairs:\n",
    "            \n",
    "                #foreign_words = foreign_sentence.split()\n",
    "                #english_words = english_sentence.split()\n",
    "\n",
    "                for f in foreign_sentence:\n",
    "                    \n",
    "                    total_f[f] = sum(translation_probabilities[e][f] for e in english_sentence)\n",
    "\n",
    "                for e in english_sentence:\n",
    "                    for f in foreign_sentence:\n",
    "                        \n",
    "                        count = translation_probabilities[e][f] / total_f[f]\n",
    "                        counts_fe[e][f] += count\n",
    "                        counts_e[e] += count\n",
    "            \n",
    "            #update probabilities\n",
    "            for e in counts_fe:\n",
    "                for f in counts_fe[e]:\n",
    "                    \n",
    "                    translation_probabilities[e][f] = counts_fe[e][f] / counts_e[e]\n",
    "        \n",
    "        self.translation_probabilities[language] = translation_probabilities\n",
    "\n",
    "\n",
    "    def initialize_translation_probabilities(self, sentence_pairs):\n",
    "\n",
    "        translation_probabilities = defaultdict(lambda: defaultdict(float))\n",
    "        \n",
    "        for (foreign_sentence, english_sentence) in sentence_pairs:\n",
    "            \n",
    "            #foreign_words = foreign_sentence.split()\n",
    "            #english_words = english_sentence.split()\n",
    "            \n",
    "            uniform_probability = 1 / len(english_sentence)\n",
    "            \n",
    "            for f in foreign_sentence:\n",
    "                for e in english_sentence:\n",
    "                    \n",
    "                    translation_probabilities[e][f] = uniform_probability\n",
    "        \n",
    "        return translation_probabilities\n",
    "\n",
    "\n",
    "    def find_top_translations(self, word, language, n=10):\n",
    "        \n",
    "        if language not in self.languages:\n",
    "            \n",
    "            print(f'Language \\\"{language}\\\" not found.')\n",
    "            return\n",
    "       \n",
    "        translations = [(f, prob) for f, prob in self.translation_probabilities[language][word].items()]\n",
    "        translations.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if len(translations) == 0:\n",
    "            \n",
    "            print(f'Translation of \\\"{word}\\\" not found.')\n",
    "            return\n",
    "\n",
    "        return translations[:n]\n",
    "    \n",
    "\n",
    "    def decode(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_model = TranslationModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_model.add_sentence_pairs(swe_eng_file_path, eng_swe_file_path, 'se')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('europeiska', 0.8358811726335589),\n",
       " ('europeisk', 0.07793251129315405),\n",
       " ('den', 0.01234342827816025),\n",
       " ('i', 0.01145802549009602),\n",
       " ('att', 0.0073175106656019956),\n",
       " ('en', 0.0064164076171784445),\n",
       " ('till', 0.006403088625106279),\n",
       " ('det', 0.005713358673491642),\n",
       " (',', 0.005560543111116395),\n",
       " ('för', 0.0047190385350298)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_model.find_top_translations('european', 'se')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_model.add_sentence_pairs(ger_eng_file_path, eng_ger_file_path, 'de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('europäischen', 0.6583104427978661),\n",
       " ('europäische', 0.2994256203430025),\n",
       " ('der', 0.015614335052903423),\n",
       " ('die', 0.006707869728434155),\n",
       " (',', 0.0046853348392669105),\n",
       " ('den', 0.002090016898065694),\n",
       " ('in', 0.002076244515776026),\n",
       " ('.', 0.001742672454766282),\n",
       " ('union', 0.0017056204984250926),\n",
       " ('das', 0.0015701739731939042)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_model.find_top_translations('european', 'de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_model.add_sentence_pairs(fre_eng_file_path, eng_fre_file_path, 'fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('européenne', 0.46255524097583534),\n",
       " ('européen', 0.2740681214755836),\n",
       " ('l', 0.07615189999366892),\n",
       " ('&apos;', 0.06673961686785357),\n",
       " ('de', 0.043416375856401186),\n",
       " (',', 0.012650462639585984),\n",
       " ('le', 0.011782554822424903),\n",
       " ('la', 0.008875011271880097),\n",
       " ('au', 0.00688588253565706),\n",
       " ('d', 0.005003206485329585)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_model.find_top_translations('european', 'fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dedoding is challenging because we have a very large search space. From the outputs above, we see that for each foreign word there are multiple possible translations in English. In the IBM model 1 word alignments are assumed to be indepent of each other so the order of thr words and context is not considered which leads to less natural sentence translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
