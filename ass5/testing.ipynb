{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Module 5: Natural language processing**\n",
    "## DAT410\n",
    "\n",
    "### Group 29 \n",
    "### David Laessker, 980511-5012, laessker@chalmers.se\n",
    "\n",
    "### Oskar Palmgren, 010529-4714, oskarpal@chalmers.se\n",
    "\n",
    "\n",
    "\n",
    "We hereby declare that we have both actively participated in solving every exercise. All solutions are entirely our own work, without having taken part of other solutions.\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Reading and reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Like speech recognition and image recognition?\n",
    "\n",
    "b) Systems that are rule-based explicitly use linguistic rules and dictionaries, while neural systems learn these linguistic patterns from large datasets. Both approaches aim to accurately translate languages by mapping structures and meanings, but through different means.\n",
    "\n",
    "c) Maybe smaller datasets? Modern neural systems may not capture the grammatical patterns in the language with scarce data. A rule based system will therefore offer more predictable and interpretable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "swe_eng_file_path = 'data/europarl-v7.sv-en.lc.sv'\n",
    "eng_swe_file_path = 'data\\europarl-v7.sv-en.lc.en'\n",
    "\n",
    "ger_eng_file_path = 'data\\europarl-v7.de-en.lc.de'\n",
    "eng_ger_file_path = 'data\\europarl-v7.de-en.lc.en'\n",
    "\n",
    "fre_eng_file_path = 'data\\europarl-v7.fr-en.lc.fr'\n",
    "eng_fre_file_path = 'data\\europarl-v7.fr-en.lc.en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency(file):\n",
    "    \n",
    "    word_counter = Counter()\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "    \n",
    "        for line in f:\n",
    "            \n",
    "            words = line.strip().split()\n",
    "            word_counter.update(words)\n",
    "    \n",
    "    return word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 9648),\n",
       " ('att', 9181),\n",
       " (',', 8876),\n",
       " ('och', 7038),\n",
       " ('i', 5949),\n",
       " ('det', 5687),\n",
       " ('som', 5028),\n",
       " ('för', 4959),\n",
       " ('av', 4013),\n",
       " ('är', 3840)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swe_frequency = word_frequency(swe_eng_file_path)\n",
    "\n",
    "swe_top_10 = swe_frequency.most_common(10)\n",
    "\n",
    "swe_top_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 58790),\n",
       " (',', 42043),\n",
       " ('.', 29542),\n",
       " ('of', 28406),\n",
       " ('to', 26842),\n",
       " ('and', 21459),\n",
       " ('in', 18485),\n",
       " ('is', 13331),\n",
       " ('that', 13219),\n",
       " ('a', 13090)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_frequency1 = word_frequency(eng_swe_file_path)\n",
    "eng_frequency2 = word_frequency(eng_ger_file_path)\n",
    "eng_frequency3 = word_frequency(eng_fre_file_path)\n",
    "\n",
    "eng_total_frequency = eng_frequency1 + eng_frequency2 + eng_frequency3\n",
    "\n",
    "eng_top_10 = eng_total_frequency.most_common(10)\n",
    "\n",
    "eng_top_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 18549),\n",
       " ('die', 10521),\n",
       " ('.', 9733),\n",
       " ('der', 9374),\n",
       " ('und', 7028),\n",
       " ('in', 4175),\n",
       " ('zu', 3168),\n",
       " ('den', 2976),\n",
       " ('wir', 2863),\n",
       " ('daß', 2738)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ger_frequency = word_frequency(ger_eng_file_path)\n",
    "\n",
    "ger_top_10 = ger_frequency.most_common(10)\n",
    "\n",
    "ger_top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('&apos;', 16729),\n",
       " (',', 15402),\n",
       " ('de', 14520),\n",
       " ('la', 9746),\n",
       " ('.', 9734),\n",
       " ('et', 6619),\n",
       " ('l', 6536),\n",
       " ('le', 6174),\n",
       " ('les', 5585),\n",
       " ('à', 5500)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fre_frequency = word_frequency(fre_eng_file_path)\n",
    "\n",
    "fre_top_10 = fre_frequency.most_common(10)\n",
    "\n",
    "fre_top_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**need to remove punctuations, other symbols such as apostophes?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of speaker: 0.0000193\n",
      "Probability of zebra: 0.0\n"
     ]
    }
   ],
   "source": [
    "eur_parl_frequency = swe_frequency + fre_frequency + ger_frequency + eng_total_frequency\n",
    "\n",
    "words_amount = sum(eur_parl_frequency.values())\n",
    "\n",
    "#print(words_amount)\n",
    "#print(eur_parl_frequency['speaker'])\n",
    "#print(eur_parl_frequency['zebra'])\n",
    "\n",
    "speaker_probability = eur_parl_frequency['speaker'] / words_amount\n",
    "zebra_probability = eur_parl_frequency['zebra'] / words_amount\n",
    "\n",
    "# calculate probability for \"speaker\" and \"zebra\"\n",
    "print(f'Probability of speaker: {speaker_probability:.7f}')\n",
    "print(f'Probability of zebra: {zebra_probability}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    '''\n",
    "    Reads the file, each sentence line by line and splits the words from the sentences. \n",
    "    Returns a list of lists with words from each sentence\n",
    "    '''\n",
    "    \n",
    "    sentences_list = []\n",
    "\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "    \n",
    "        for line in f:\n",
    "            \n",
    "            words = line.strip().split()\n",
    "            sentences_list.append(words)\n",
    "    \n",
    "    return sentences_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class BigramModel:\n",
    "    def __init__(self):\n",
    "        self.bigram_counts = defaultdict(Counter)\n",
    "        self.starting_words = []\n",
    "\n",
    "    def train(self, sentence_list):\n",
    "        # Preprocess the text into words\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            self.starting_words.append(sentence[0])\n",
    "        \n",
    "        # Count bigrams in the text\n",
    "            for i in range(len(sentence) - 1):\n",
    "                self.bigram_counts[sentence[i]][sentence[i+1]] += 1\n",
    "        \n",
    "\n",
    "    def predict_next_word(self, word):\n",
    "        if word not in self.bigram_counts:\n",
    "            return None\n",
    "        next_words = self.bigram_counts[word]\n",
    "        total_counts = sum(next_words.values())\n",
    "        # Create a weighted choice among the next possible words\n",
    "        weighted_choices = [(w, count / total_counts) for w, count in next_words.items()]\n",
    "        return random.choices([w for w, _ in weighted_choices], [count for _, count in weighted_choices])[0]\n",
    "\n",
    "    def generate_text(self, start_word, length=10):\n",
    "        \n",
    "        #print(self.starting_words)\n",
    "        #print(self.bigram_counts['jag'])\n",
    "\n",
    "\n",
    "        if start_word.lower() not in self.bigram_counts and not self.starting_words:\n",
    "            return \"Model not trained or start word not in corpus.\"\n",
    "        \n",
    "        if start_word.lower() in self.bigram_counts:\n",
    "            current_word = start_word.lower()\n",
    "\n",
    "        else:\n",
    "            current_word = random.choice(self.starting_words)\n",
    "        \n",
    "        \n",
    "        generated_text = [current_word]\n",
    "        \n",
    "        for _ in range(length - 1):\n",
    "            next_word = self.predict_next_word(current_word)\n",
    "            if next_word is None:\n",
    "                break  # End if no next word is found\n",
    "            generated_text.append(next_word)\n",
    "            current_word = next_word\n",
    "        return ' '.join(generated_text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jag väntar också gör att skapa en fråga som orienterar\n"
     ]
    }
   ],
   "source": [
    "swe_sentences = read_file(swe_eng_file_path)\n",
    "\n",
    "model = BigramModel()\n",
    "model.train(swe_sentences)\n",
    "\n",
    "start_word = \"jag\"\n",
    "generated_text = model.generate_text(start_word, 10)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def createBigram(data):\n",
    "   listOfBigrams = []\n",
    "   bigramCounts = {}\n",
    "   unigramCounts = {}\n",
    "   for i in range(len(data)-1):\n",
    "      if i < len(data) - 1 and data[i+1].islower():\n",
    "\n",
    "         listOfBigrams.append((data[i], data[i + 1]))\n",
    "\n",
    "         if (data[i], data[i+1]) in bigramCounts:\n",
    "            bigramCounts[(data[i], data[i + 1])] += 1\n",
    "         else:\n",
    "            bigramCounts[(data[i], data[i + 1])] = 1\n",
    "\n",
    "      if data[i] in unigramCounts:\n",
    "         unigramCounts[data[i]] += 1\n",
    "      else:\n",
    "         unigramCounts[data[i]] = 1\n",
    "   return listOfBigrams, unigramCounts, bigramCounts\n",
    "\n",
    "\n",
    "def calcBigramProb(listOfBigrams, unigramCounts, bigramCounts):\n",
    "    listOfProb = {}\n",
    "    for bigram in listOfBigrams:\n",
    "        word1 = bigram[0]\n",
    "        word2 = bigram[1]\n",
    "        listOfProb[bigram] = (bigramCounts.get(bigram))/(unigramCounts.get(word1))\n",
    "    return listOfProb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'islower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m listOfBigrams, unigramCounts, bigramCounts \u001b[38;5;241m=\u001b[39m \u001b[43mcreateBigram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m All the possible Bigrams are \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(listOfBigrams)\n",
      "Cell \u001b[1;32mIn[78], line 6\u001b[0m, in \u001b[0;36mcreateBigram\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      4\u001b[0m unigramCounts \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 6\u001b[0m    \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mislower\u001b[49m():\n\u001b[0;32m      8\u001b[0m       listOfBigrams\u001b[38;5;241m.\u001b[39mappend((data[i], data[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m     10\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m (data[i], data[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;129;01min\u001b[39;00m bigramCounts:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'islower'"
     ]
    }
   ],
   "source": [
    "listOfBigrams, unigramCounts, bigramCounts = createBigram(words_list[0])\n",
    "\n",
    "print(\"\\n All the possible Bigrams are \")\n",
    "print(listOfBigrams)\n",
    "\n",
    "print(\"\\n Bigrams along with their frequency \")\n",
    "print(bigramCounts)\n",
    "\n",
    "print(\"\\n Unigrams along with their frequency \")\n",
    "print(unigramCounts)\n",
    "\n",
    "bigramProb = calcBigramProb(listOfBigrams, unigramCounts, bigramCounts)\n",
    "\n",
    "print(\"\\n Bigrams along with their probability \")\n",
    "print(bigramProb)\n",
    "inputList=\"This is my cat\"\n",
    "splt=inputList.split()\n",
    "outputProb1 = 1\n",
    "bilist=[]\n",
    "bigrm=[]\n",
    "\n",
    "for i in range(len(splt) - 1):\n",
    "    if i < len(splt) - 1:\n",
    "\n",
    "        bilist.append((splt[i], splt[i + 1]))\n",
    "\n",
    "print(\"\\n The bigrams in given sentence are \")\n",
    "print(bilist)\n",
    "for i in range(len(bilist)):\n",
    "    if bilist[i] in bigramProb:\n",
    "\n",
    "        outputProb1 *= bigramProb[bilist[i]]\n",
    "    else:\n",
    "\n",
    "        outputProb1 *= 0\n",
    "print('\\n' + 'Probablility of sentence \\\"This is my cat\\\" = ' + str(outputProb1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
